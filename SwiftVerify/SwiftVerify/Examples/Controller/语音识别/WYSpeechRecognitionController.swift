//
//  WYSpeechRecognitionController.swift
//  WYBasisKitTest
//
//  Created by å®˜äºº on 2023/6/8.
//  å‚è€ƒï¼šhttps://blog.51cto.com/u_11643026/6273204

import UIKit
import Speech

class WYSpeechRecognitionController: UIViewController {
    
    @IBOutlet weak var textView: UITextView!
    
    @IBOutlet weak var voiceView: UIButton!
    
    // åœ¨è¿›è¡Œè¯­éŸ³è¯†åˆ«ä¹‹å‰ï¼Œä½ å¿…é¡»è·å¾—ç”¨æˆ·çš„ç›¸åº”æˆæƒï¼Œå› ä¸ºè¯­éŸ³è¯†åˆ«å¹¶ä¸æ˜¯åœ¨iOSè®¾å¤‡æœ¬åœ°è¿›è¡Œè¯†åˆ«ï¼Œè€Œæ˜¯åœ¨è‹¹æœçš„ä¼ºæœå™¨ä¸Šè¿›è¡Œè¯†åˆ«çš„ã€‚æ‰€æœ‰çš„è¯­éŸ³æ•°æ®éƒ½éœ€è¦ä¼ ç»™è‹¹æœçš„åå°æœåŠ¡å™¨è¿›è¡Œå¤„ç†ã€‚å› æ­¤å¿…é¡»å¾—åˆ°ç”¨æˆ·çš„æˆæƒã€‚
    
    // åˆ›å»ºè¯­éŸ³è¯†åˆ«å™¨ï¼ŒæŒ‡å®šè¯­éŸ³è¯†åˆ«çš„è¯­è¨€ç¯å¢ƒ locale ,å°†æ¥ä¼šè½¬åŒ–ä¸ºä»€ä¹ˆè¯­è¨€ï¼Œè¿™é‡Œæ˜¯ä½¿ç”¨çš„å½“å‰åŒºåŸŸï¼Œé‚£è‚¯å®šå°±æ˜¯ç®€ä½“ä¸­æ–‡å•¦
    private let speechRecognizer = SFSpeechRecognizer(locale: Locale.autoupdatingCurrent)
    
    // ä½¿ç”¨ identifier è¿™é‡Œè®¾ç½®çš„åŒºåŸŸæ˜¯å°æ¹¾ï¼Œå°†æ¥ä¼šè½¬åŒ–ä¸ºç¹ä½“æ±‰è¯­
    //    private let speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: "zh-TW"))
    
    // å‘èµ·è¯­éŸ³è¯†åˆ«è¯·æ±‚ï¼Œä¸ºè¯­éŸ³è¯†åˆ«å™¨æŒ‡å®šä¸€ä¸ªéŸ³é¢‘è¾“å…¥æºï¼Œè¿™é‡Œæ˜¯åœ¨éŸ³é¢‘ç¼“å†²å™¨ä¸­æä¾›çš„è¯†åˆ«è¯­éŸ³ã€‚
    // é™¤ SFSpeechAudioBufferRecognitionRequest ä¹‹å¤–è¿˜åŒ…æ‹¬ï¼š
    // SFSpeechRecognitionRequest  ä»éŸ³é¢‘æºè¯†åˆ«è¯­éŸ³çš„è¯·æ±‚ã€‚
    // SFSpeechURLRecognitionRequest åœ¨å½•åˆ¶çš„éŸ³é¢‘æ–‡ä»¶ä¸­è¯†åˆ«è¯­éŸ³çš„è¯·æ±‚ã€‚
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    
    // è¯­éŸ³è¯†åˆ«ä»»åŠ¡ï¼Œå¯ç›‘æ§è¯†åˆ«è¿›åº¦ï¼Œé€šè¿‡ä»–å¯ä»¥å–æ¶ˆæˆ–ç»ˆæ­¢å½“å‰çš„è¯­éŸ³è¯†åˆ«ä»»åŠ¡
    private var recognitionTask: SFSpeechRecognitionTask?
    
    // è¯­éŸ³å¼•æ“ï¼Œè´Ÿè´£æä¾›å½•éŸ³è¾“å…¥
    private let audioEngine: AVAudioEngine? = AVAudioEngine()
    
    // è®°å½•æ¯æ¬¡è¯†åˆ«åˆ°çš„æ–‡å­—
    private var audioToTexts: [String] = []
    
    override func viewDidLoad() {
        super.viewDidLoad()
        // Do any additional setup after loading the view.
        
        // ç½‘ç»œç›‘å¬
        WYNetworkStatus.listening("SpeechRecognition") { nwpath in
            
            // âœ… æ˜¯å¦å·²è¿æ¥ç½‘ç»œ
            if WYNetworkStatus.isReachable {
                WYLogManager.output("âœ… ç½‘ç»œè¿æ¥æ­£å¸¸")
            }
            
            // âŒ æ˜¯å¦æ— æ³•è¿æ¥
            if WYNetworkStatus.isNotReachable {
                WYLogManager.output("âŒ å½“å‰æ²¡æœ‰ç½‘ç»œè¿æ¥ï¼ˆå¯èƒ½æ˜¯é£è¡Œæ¨¡å¼ã€æ–­ç½‘æˆ–ä¿¡å·å¤ªå·®ï¼‰")
            }
            
            // âš ï¸ æ˜¯å¦éœ€è¦é¢å¤–æ­¥éª¤ï¼ˆå¦‚ç™»å½•è®¤è¯ï¼‰
            if WYNetworkStatus.requiresConnection {
                WYLogManager.output("âš ï¸ ç½‘ç»œéœ€è¦å»ºç«‹è¿æ¥ï¼ˆå¯èƒ½éœ€è¦è®¤è¯ç™»å½•ï¼‰")
            }
            
            // ğŸ“± æ˜¯å¦èœ‚çªæ•°æ®ç½‘ç»œ
            if WYNetworkStatus.isReachableOnCellular {
                WYLogManager.output("ğŸ“± å½“å‰ä½¿ç”¨èœ‚çªç§»åŠ¨ç½‘ç»œï¼ˆ4G/5G æ•°æ®æµé‡ï¼‰")
            }
            
            // ğŸ“¶ æ˜¯å¦ Wi-Fi
            if WYNetworkStatus.isReachableOnWiFi {
                WYLogManager.output("ğŸ“¶ å½“å‰é€šè¿‡ Wi-Fi è¿æ¥ç½‘ç»œ")
            }
            
            // ğŸ–¥ï¸ æ˜¯å¦æœ‰çº¿ç½‘ç»œ
            if WYNetworkStatus.isReachableOnWiredEthernet {
                WYLogManager.output("ğŸ–¥ï¸ å½“å‰ä½¿ç”¨æœ‰çº¿ç½‘ç»œï¼ˆä¾‹å¦‚ Lightning è½¬ç½‘çº¿é€‚é…å™¨ï¼‰")
            }
            
            // ğŸ›¡ï¸ æ˜¯å¦ VPN è¿æ¥
            if WYNetworkStatus.isReachableOnVPN {
                WYLogManager.output("ğŸ›¡ï¸ å½“å‰é€šè¿‡ VPN è¿æ¥ï¼ˆåŠ å¯†é€šé“ï¼Œå¯èƒ½æ”¹å˜å‡ºå£ IPï¼‰")
            }
            
            // ğŸ” æ˜¯å¦æœ¬åœ°å›ç¯æ¥å£
            if WYNetworkStatus.isLoopback {
                WYLogManager.output("ğŸ” å½“å‰ç½‘ç»œæ˜¯æœ¬åœ°å›ç¯æ¥å£ï¼ˆä»…é™è®¾å¤‡å†…éƒ¨é€šä¿¡ï¼‰")
            }
            
            // ğŸ’° æ˜¯å¦æ˜‚è´µè¿æ¥ï¼ˆèœ‚çªæˆ–çƒ­ç‚¹ï¼‰
            if WYNetworkStatus.isExpensive {
                WYLogManager.output("ğŸ’° å½“å‰ç½‘ç»œè¿æ¥æ˜‚è´µï¼ˆä¾‹å¦‚èœ‚çªæ•°æ®æˆ–ä¸ªäººçƒ­ç‚¹ï¼‰")
            }
            
            // ğŸŒ æ˜¯å¦å…¶ä»–(æœªçŸ¥ç±»å‹)
            if WYNetworkStatus.isReachableOnOther {
                WYLogManager.output("ğŸŒ å½“å‰æ˜¯å…¶ä»–(æœªçŸ¥ç±»å‹)çš„ç½‘ç»œæ¥å£ï¼ˆä¸åœ¨å¸¸è§„åˆ†ç±»ä¸­ï¼‰")
            }
            
            // ğŸŒ æ˜¯å¦æ”¯æŒ IPv4
            if WYNetworkStatus.supportsIPv4 {
                WYLogManager.output("ğŸŒ å½“å‰ç½‘ç»œæ”¯æŒ IPv4 åè®®")
            }
            
            // ğŸŒ æ˜¯å¦æ”¯æŒ IPv6
            if WYNetworkStatus.supportsIPv6 {
                WYLogManager.output("ğŸŒ å½“å‰ç½‘ç»œæ”¯æŒ IPv6 åè®®")
            }
            
            // ğŸ§© å½“å‰ç½‘ç»œçŠ¶æ€å€¼
            let status = nwpath.status
            switch status {
            case .satisfied:
                WYActivity.showInfo("ğŸŸ¢ å½“å‰ç½‘ç»œçŠ¶æ€ï¼šå·²è¿æ¥ï¼ˆsatisfiedï¼‰")
            case .unsatisfied:
                WYActivity.showInfo("ğŸ”´ å½“å‰ç½‘ç»œçŠ¶æ€ï¼šæœªè¿æ¥ï¼ˆunsatisfiedï¼‰")
            case .requiresConnection:
                WYActivity.showInfo("ğŸŸ¡ å½“å‰ç½‘ç»œçŠ¶æ€ï¼šéœ€è¦é¢å¤–è¿æ¥æ­¥éª¤ï¼ˆrequiresConnectionï¼‰")
            @unknown default:
                WYActivity.showInfo("âšªï¸ å½“å‰ç½‘ç»œçŠ¶æ€ï¼šæœªçŸ¥")
            }
        }
        
        // è¾“å‡ºä¸€ä¸‹è¯­éŸ³è¯†åˆ«å™¨æ”¯æŒçš„åŒºåŸŸï¼Œå°±æ˜¯ä¸Šè¾¹åˆå§‹åŒ–SFSpeechRecognizer æ—¶ locale æ‰€éœ€è¦çš„ identifier
        WYLogManager.output(SFSpeechRecognizer.supportedLocales())
        
        voiceView.isEnabled = false
        // è®¾ç½®è¯­éŸ³è¯†åˆ«å™¨ä»£ç†
        speechRecognizer?.delegate = self
        
        // è¦æ±‚ç”¨æˆ·æˆäºˆæ‚¨çš„åº”ç”¨è®¸å¯æ¥æ‰§è¡Œè¯­éŸ³è¯†åˆ«ã€‚
        wy_authorizeSpeechRecognition { authorized in
            OperationQueue.main.addOperation() {
                self.textView.isUserInteractionEnabled = false
                self.voiceView.isEnabled = authorized
            }
        }
    }
    
    func startRecordingPersonSpeech() {
        // æ£€æŸ¥ recognitionTask ä»»åŠ¡æ˜¯å¦å¤„äºè¿è¡ŒçŠ¶æ€ã€‚å¦‚æœæ˜¯ï¼Œå–æ¶ˆä»»åŠ¡å¼€å§‹æ–°çš„ä»»åŠ¡
        if recognitionTask != nil {
            // å–æ¶ˆå½“å‰è¯­éŸ³è¯†åˆ«ä»»åŠ¡ã€‚
            recognitionTask?.cancel()
            recognitionTask = nil
        }
        
        // å»ºç«‹ä¸€ä¸ªAVAudioSession ç”¨äºå½•éŸ³
        let audioSession = AVAudioSession.sharedInstance()
        do {
            // category è®¾ç½®ä¸º record,å½•éŸ³
            try audioSession.setCategory(AVAudioSession.Category.record)
            // mode è®¾ç½®ä¸º measurement
            try audioSession.setMode(AVAudioSession.Mode.measurement)
            // å¼€å¯ audioSession
            try audioSession.setActive(true, options: .notifyOthersOnDeactivation)
        } catch {
            WYLogManager.output("audioSession properties weren't set because of an error.")
        }
        
        // åˆå§‹åŒ–RecognitionRequestï¼Œåœ¨åè¾¹æˆ‘ä»¬ä¼šç”¨å®ƒå°†å½•éŸ³æ•°æ®è½¬å‘ç»™è‹¹æœæœåŠ¡å™¨
        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        
        // æ£€æŸ¥ iPhone æ˜¯å¦æœ‰æœ‰æ•ˆçš„å½•éŸ³è®¾å¤‡
        guard let inputNode = audioEngine?.inputNode else {
            fatalError("Audio engine has no input node")
        }
        
        //
        guard let recognitionRequest = recognitionRequest else {
            fatalError("Unable to create an SFSpeechAudioBufferRecognitionRequest object")
        }
        // åœ¨ç”¨æˆ·è¯´è¯çš„åŒæ—¶ï¼Œå°†è¯†åˆ«ç»“æœåˆ†æ‰¹æ¬¡è¿”å›
        recognitionRequest.shouldReportPartialResults = true
        
        // æ·»åŠ æ ‡ç‚¹ç¬¦å·
        if #available(iOS 16, *) {
            recognitionRequest.addsPunctuation = true
        } else {
            // iOS16ä»¥ä¸‹éœ€è¦è‡ªå·±å¤„ç†æ ‡ç‚¹ç¬¦å·ï¼Œå»ºè®®å¯ä»¥å‚è€ƒï¼šPaddleSpeech
        }
        
        // é˜²æ­¢é€šè¿‡ç½‘ç»œå‘é€éŸ³é¢‘ï¼Œè¯†åˆ«å°†ä¸å†é‚£ä¹ˆå‡†ç¡®
        if speechRecognizer?.supportsOnDeviceRecognition ?? false {
            recognitionRequest.requiresOnDeviceRecognition = true
        }
        
        // ä½¿ç”¨recognitionTaskæ–¹æ³•å¼€å§‹è¯†åˆ«ï¼Œè¿™é‡Œæ¨èä»£ç†å®ç°æ–¹å¼ï¼Œé—­åŒ…æ–¹å¼æ— æ³•å°†å·²ç»è¯†åˆ«åˆ°çš„æ–‡æœ¬å’Œæ–°è¯†åˆ«åˆ°çš„æ–‡æœ¬è¿æ¥èµ·æ¥
        recognitionTask = speechRecognizer?.recognitionTask(with: recognitionRequest, delegate: self)
        
        /*
         recognitionTask = speechRecognizer?.recognitionTask(with: recognitionRequest, resultHandler: { [weak self] (result, error) in
 
             guard let self = self else { return }
             // ç”¨äºæ£€æŸ¥è¯†åˆ«æ˜¯å¦ç»“æŸ
             var isFinal = false
             // å¦‚æœ result ä¸æ˜¯ nil,
             if result != nil {
                 // å°† textView.text è®¾ç½®ä¸º result çš„æœ€ä½³éŸ³è¯‘
                 textView.text = result?.bestTranscription.formattedString ?? ""
                 
                 // å¦‚æœ result æ˜¯æœ€ç»ˆï¼Œå°† isFinal è®¾ç½®ä¸º true
                 isFinal = (result?.isFinal)!
             }
 
             // å¦‚æœæ²¡æœ‰é”™è¯¯å‘ç”Ÿï¼Œæˆ–è€… result å·²ç»ç»“æŸï¼Œåœæ­¢audioEngine å½•éŸ³ï¼Œç»ˆæ­¢ recognitionRequest å’Œ recognitionTask
             if error != nil || isFinal {
                 audioEngine?.stop()
                 inputNode.removeTap(onBus: 0)
 
                 self.recognitionRequest = nil
                 recognitionTask = nil
                 // å¼€å§‹å½•éŸ³æŒ‰é’®å¯ç”¨
                 voiceView.isEnabled = true
             }
         })
         */
        
        // å‘recognitionRequeståŠ å…¥ä¸€ä¸ªéŸ³é¢‘è¾“å…¥
        let recordingFormat = inputNode.outputFormat(forBus: 0)
        inputNode.installTap(onBus: 0, bufferSize: 1024, format: recordingFormat) { [weak self] (buffer, when) in
            self?.recognitionRequest?.append(buffer)
        }
        
        audioEngine?.prepare()
        
        do {
            // å¼€å§‹å½•éŸ³
            try audioEngine?.start()
        } catch {
            WYLogManager.output("audioEngine couldn't start because of an error.")
        }
        textView.text = "è¯·è®²è¯..."
    }
    
    @IBAction func startRecording(_ sender: UIButton) {
        
        if audioEngine?.isRunning ?? false {
            // åœæ­¢å½•éŸ³
            audioEngine?.stop()
            // è¡¨ç¤ºéŸ³é¢‘æºå·²å®Œæˆï¼Œå¹¶ä¸”ä¸ä¼šå†å°†éŸ³é¢‘é™„åŠ åˆ°è¯†åˆ«è¯·æ±‚ã€‚
            recognitionRequest?.endAudio()
            voiceView.isEnabled = false
            voiceView.setTitle("è¯­éŸ³è¯†åˆ«", for: .normal)
        } else {
            startRecordingPersonSpeech()
            voiceView.setTitle("ç»“æŸ", for: .normal)
        }
    }
    
    deinit {
        WYNetworkStatus.stopListening("SpeechRecognition")
    }
    
    /*
     // MARK: - Navigation
     
     // In a storyboard-based application, you will often want to do a little preparation before navigation
     override func prepare(for segue: UIStoryboardSegue, sender: Any?) {
     // Get the new view controller using segue.destination.
     // Pass the selected object to the new view controller.
     }
     */
    
}

extension WYSpeechRecognitionController: SFSpeechRecognizerDelegate, SFSpeechRecognitionTaskDelegate {
    
    // Called when the task first detects speech in the source audio
    func speechRecognitionDidDetectSpeech(_ task: SFSpeechRecognitionTask) {
        WYLogManager.output("Called when the task first detects speech in the source audio")
    }
    
    
    // Called for all recognitions, including non-final hypothesis
    func speechRecognitionTask(_ task: SFSpeechRecognitionTask, didHypothesizeTranscription transcription: SFTranscription) {
        // åœ¨è¿™é‡Œå®ç°å³æ—¶è½¬è¯‘æ•ˆæœ
        textView.text = audioToTexts.joined().appending(transcription.formattedString)
    }
    
    
    // Called only for final recognitions of utterances. No more about the utterance will be reported
    func speechRecognitionTask(_ task: SFSpeechRecognitionTask, didFinishRecognition recognitionResult: SFSpeechRecognitionResult) {
        // è¿™é‡Œæ˜¯è·å–æœ€ç»ˆçš„è¯†åˆ«ç»“æœï¼Œå¹¶ä¸”å°† textView.text è®¾ç½®ä¸º result çš„æœ€ä½³éŸ³è¯‘
        // æ·»åŠ æ ‡ç‚¹ç¬¦å·
        var symbol: String = ""
        if #available(iOS 16, *) {
        } else {
            symbol = ","
        }
        audioToTexts.append(recognitionResult.bestTranscription.formattedString.appending(symbol))
        textView.text = audioToTexts.joined()
    }
    
    
    // Called when the task is no longer accepting new audio but may be finishing final processing
    func speechRecognitionTaskFinishedReadingAudio(_ task: SFSpeechRecognitionTask) {
        WYLogManager.output("Called when the task is no longer accepting new audio but may be finishing final processing")
    }
    
    
    // Called when the task has been cancelled, either by client app, the user, or the system
    func speechRecognitionTaskWasCancelled(_ task: SFSpeechRecognitionTask) {
        WYLogManager.output("Called when the task has been cancelled, either by client app, the user, or the system")
    }
    
    
    // Called when recognition of all requested utterances is finished.
    // If successfully is false, the error property of the task will contain error information
    func speechRecognitionTask(_ task: SFSpeechRecognitionTask, didFinishSuccessfully successfully: Bool) {
        
        audioEngine?.stop()
        audioEngine?.inputNode.removeTap(onBus: 0)
        recognitionRequest = nil
        recognitionTask = nil
        audioToTexts.removeAll()
        textView.text = "è¯­éŸ³è¯†åˆ«æ­¥éª¤\n1ã€æŒ‰ä¸‹ è¯­éŸ³è¯†åˆ« æŒ‰é’®\n2ã€è¯­éŸ³è¯†åˆ«(è¯´å‡ºæƒ³è¦è¯†åˆ«çš„å†…å®¹)\n3ã€æŒ‰ä¸‹ ç»“æŸ æŒ‰é’®ç»“æŸè¯­éŸ³è¯†åˆ«"
        voiceView.isEnabled = true
    }
}
