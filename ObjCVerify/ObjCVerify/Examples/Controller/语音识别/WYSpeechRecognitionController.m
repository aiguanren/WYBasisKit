//
//  WYSpeechRecognitionController.m
//  ObjCVerify
//
//  Created by guanren on 2026/1/13.
//

#import "WYSpeechRecognitionController.h"
#import <Speech/Speech.h>
#import <WYBasisKitObjC/WYBasisKitObjC.h>

// åœ¨è¿›è¡Œè¯­éŸ³è¯†åˆ«ä¹‹å‰ï¼Œä½ å¿…é¡»è·å¾—ç”¨æˆ·çš„ç›¸åº”æˆæƒï¼Œå› ä¸ºè¯­éŸ³è¯†åˆ«å¹¶ä¸æ˜¯åœ¨iOSè®¾å¤‡æœ¬åœ°è¿›è¡Œè¯†åˆ«ï¼Œè€Œæ˜¯åœ¨è‹¹æœçš„ä¼ºæœå™¨ä¸Šè¿›è¡Œè¯†åˆ«çš„ã€‚æ‰€æœ‰çš„è¯­éŸ³æ•°æ®éƒ½éœ€è¦ä¼ ç»™è‹¹æœçš„åå°æœåŠ¡å™¨è¿›è¡Œå¤„ç†ã€‚å› æ­¤å¿…é¡»å¾—åˆ°ç”¨æˆ·çš„æˆæƒã€‚

// åˆ›å»ºè¯­éŸ³è¯†åˆ«å™¨ï¼ŒæŒ‡å®šè¯­éŸ³è¯†åˆ«çš„è¯­è¨€ç¯å¢ƒ locale ,å°†æ¥ä¼šè½¬åŒ–ä¸ºä»€ä¹ˆè¯­è¨€ï¼Œè¿™é‡Œæ˜¯ä½¿ç”¨çš„å½“å‰åŒºåŸŸï¼Œé‚£è‚¯å®šå°±æ˜¯ç®€ä½“ä¸­æ–‡å•¦

@interface WYSpeechRecognitionController () <SFSpeechRecognizerDelegate, SFSpeechRecognitionTaskDelegate>

@property (weak, nonatomic) IBOutlet UITextView *textView;

@property (weak, nonatomic) IBOutlet UIButton *voiceView;

@property (nonatomic, strong) SFSpeechRecognizer *speechRecognizer;

// ä½¿ç”¨ identifier è¿™é‡Œè®¾ç½®çš„åŒºåŸŸæ˜¯å°æ¹¾ï¼Œå°†æ¥ä¼šè½¬åŒ–ä¸ºç¹ä½“æ±‰è¯­
//    @property (nonatomic, strong) SFSpeechRecognizer *speechRecognizer;

// å‘èµ·è¯­éŸ³è¯†åˆ«è¯·æ±‚ï¼Œä¸ºè¯­éŸ³è¯†åˆ«å™¨æŒ‡å®šä¸€ä¸ªéŸ³é¢‘è¾“å…¥æºï¼Œè¿™é‡Œæ˜¯åœ¨éŸ³é¢‘ç¼“å†²å™¨ä¸­æä¾›çš„è¯†åˆ«è¯­éŸ³ã€‚
// é™¤ SFSpeechAudioBufferRecognitionRequest ä¹‹å¤–è¿˜åŒ…æ‹¬ï¼š
// SFSpeechRecognitionRequest  ä»éŸ³é¢‘æºè¯†åˆ«è¯­éŸ³çš„è¯·æ±‚ã€‚
// SFSpeechURLRecognitionRequest åœ¨å½•åˆ¶çš„éŸ³é¢‘æ–‡ä»¶ä¸­è¯†åˆ«è¯­éŸ³çš„è¯·æ±‚ã€‚
@property (nonatomic, strong) SFSpeechAudioBufferRecognitionRequest *recognitionRequest;

// è¯­éŸ³è¯†åˆ«ä»»åŠ¡ï¼Œå¯ç›‘æ§è¯†åˆ«è¿›åº¦ï¼Œé€šè¿‡ä»–å¯ä»¥å–æ¶ˆæˆ–ç»ˆæ­¢å½“å‰çš„è¯­éŸ³è¯†åˆ«ä»»åŠ¡
@property (nonatomic, strong) SFSpeechRecognitionTask *recognitionTask;

// è¯­éŸ³å¼•æ“ï¼Œè´Ÿè´£æä¾›å½•éŸ³è¾“å…¥
@property (nonatomic, strong) AVAudioEngine *audioEngine;

// è®°å½•æ¯æ¬¡è¯†åˆ«åˆ°çš„æ–‡å­—
@property (nonatomic, strong) NSMutableArray<NSString *> *audioToTexts;

@end

@implementation WYSpeechRecognitionController

- (void)viewDidLoad {
    [super viewDidLoad];
    // Do any additional setup after loading the view.
    
    // ç½‘ç»œç›‘å¬
    [WYNetworkStatus listening:@"SpeechRecognition" queue:dispatch_get_main_queue() handler:^(NSInteger status) {
            
        // âœ… æ˜¯å¦å·²è¿æ¥ç½‘ç»œ
        if (WYNetworkStatus.isReachable) {
            WYLogManager.output(@"âœ… ç½‘ç»œè¿æ¥æ­£å¸¸");
        }
        
        // âŒ æ˜¯å¦æ— æ³•è¿æ¥
        if (WYNetworkStatus.isNotReachable) {
            WYLogManager.output(@"âŒ å½“å‰æ²¡æœ‰ç½‘ç»œè¿æ¥ï¼ˆå¯èƒ½æ˜¯é£è¡Œæ¨¡å¼ã€æ–­ç½‘æˆ–ä¿¡å·å¤ªå·®ï¼‰");
        }
        
        // âš ï¸ æ˜¯å¦éœ€è¦é¢å¤–æ­¥éª¤ï¼ˆå¦‚ç™»å½•è®¤è¯ï¼‰
        if (WYNetworkStatus.requiresConnection) {
            WYLogManager.output(@"âš ï¸ ç½‘ç»œéœ€è¦å»ºç«‹è¿æ¥ï¼ˆå¯èƒ½éœ€è¦è®¤è¯ç™»å½•ï¼‰");
        }
        
        // ğŸ“± æ˜¯å¦èœ‚çªæ•°æ®ç½‘ç»œ
        if (WYNetworkStatus.isReachableOnCellular) {
            WYLogManager.output(@"ğŸ“± å½“å‰ä½¿ç”¨èœ‚çªç§»åŠ¨ç½‘ç»œï¼ˆ4G/5G æ•°æ®æµé‡ï¼‰");
        }
        
        // ğŸ“¶ æ˜¯å¦ Wi-Fi
        if (WYNetworkStatus.isReachableOnWiFi) {
            WYLogManager.output(@"ğŸ“¶ å½“å‰é€šè¿‡ Wi-Fi è¿æ¥ç½‘ç»œ");
        }
        
        // ğŸ–¥ï¸ æ˜¯å¦æœ‰çº¿ç½‘ç»œ
        if (WYNetworkStatus.isReachableOnWiredEthernet) {
            WYLogManager.output(@"ğŸ–¥ï¸ å½“å‰ä½¿ç”¨æœ‰çº¿ç½‘ç»œï¼ˆä¾‹å¦‚ Lightning è½¬ç½‘çº¿é€‚é…å™¨ï¼‰");
        }
        
        // ğŸ›¡ï¸ æ˜¯å¦ VPN è¿æ¥
        if (WYNetworkStatus.isReachableOnVPN) {
            WYLogManager.output(@"ğŸ›¡ï¸ å½“å‰é€šè¿‡ VPN è¿æ¥ï¼ˆåŠ å¯†é€šé“ï¼Œå¯èƒ½æ”¹å˜å‡ºå£ IPï¼‰");
        }
        
        // ğŸ” æ˜¯å¦æœ¬åœ°å›ç¯æ¥å£
        if (WYNetworkStatus.isLoopback) {
            WYLogManager.output(@"ğŸ” å½“å‰ç½‘ç»œæ˜¯æœ¬åœ°å›ç¯æ¥å£ï¼ˆä»…é™è®¾å¤‡å†…éƒ¨é€šä¿¡ï¼‰");
        }
        
        // ğŸ’° æ˜¯å¦æ˜‚è´µè¿æ¥ï¼ˆèœ‚çªæˆ–çƒ­ç‚¹ï¼‰
        if (WYNetworkStatus.isExpensive) {
            WYLogManager.output(@"ğŸ’° å½“å‰ç½‘ç»œè¿æ¥æ˜‚è´µï¼ˆä¾‹å¦‚èœ‚çªæ•°æ®æˆ–ä¸ªäººçƒ­ç‚¹ï¼‰");
        }
        
        // ğŸŒ æ˜¯å¦å…¶ä»–(æœªçŸ¥ç±»å‹)
        if (WYNetworkStatus.isReachableOnOther) {
            WYLogManager.output(@"ğŸŒ å½“å‰æ˜¯å…¶ä»–(æœªçŸ¥ç±»å‹)çš„ç½‘ç»œæ¥å£ï¼ˆä¸åœ¨å¸¸è§„åˆ†ç±»ä¸­ï¼‰");
        }
        
        // ğŸŒ æ˜¯å¦æ”¯æŒ IPv4
        if (WYNetworkStatus.supportsIPv4) {
            WYLogManager.output(@"ğŸŒ å½“å‰ç½‘ç»œæ”¯æŒ IPv4 åè®®");
        }
        
        // ğŸŒ æ˜¯å¦æ”¯æŒ IPv6
        if (WYNetworkStatus.supportsIPv6) {
            WYLogManager.output(@"ğŸŒ å½“å‰ç½‘ç»œæ”¯æŒ IPv6 åè®®");
        }
        
        // ğŸ§© å½“å‰ç½‘ç»œçŠ¶æ€å€¼
        NSArray <NSString *>*statusTips = @[@"ğŸŸ¢ å½“å‰ç½‘ç»œçŠ¶æ€ï¼šå·²è¿æ¥ï¼ˆsatisfiedï¼‰",
                                @"ğŸ”´ å½“å‰ç½‘ç»œçŠ¶æ€ï¼šæœªè¿æ¥ï¼ˆunsatisfiedï¼‰",
                                @"ğŸŸ¡ å½“å‰ç½‘ç»œçŠ¶æ€ï¼šéœ€è¦é¢å¤–è¿æ¥æ­¥éª¤ï¼ˆrequiresConnectionï¼‰",
                                @"âšªï¸ å½“å‰ç½‘ç»œçŠ¶æ€ï¼šæœªçŸ¥"];
        
        [WYActivity showInfo:statusTips[status]];
    }];
    
    // è¾“å‡ºä¸€ä¸‹è¯­éŸ³è¯†åˆ«å™¨æ”¯æŒçš„åŒºåŸŸï¼Œå°±æ˜¯ä¸Šè¾¹åˆå§‹åŒ–SFSpeechRecognizer æ—¶ locale æ‰€éœ€è¦çš„ identifier
    WYLog(@"%@", [SFSpeechRecognizer supportedLocales]);
    
    self.voiceView.enabled = NO;
    
    // åˆå§‹åŒ–è¯­éŸ³è¯†åˆ«å™¨
    self.speechRecognizer = [[SFSpeechRecognizer alloc] initWithLocale:NSLocale.autoupdatingCurrentLocale];
    
    // ä½¿ç”¨ identifier è¿™é‡Œè®¾ç½®çš„åŒºåŸŸæ˜¯å°æ¹¾ï¼Œå°†æ¥ä¼šè½¬åŒ–ä¸ºç¹ä½“æ±‰è¯­
//    self.speechRecognizer = [[SFSpeechRecognizer alloc] initWithLocale:[NSLocale localeWithLocaleIdentifier:@"zh-TW"]];
    
    // è®¾ç½®è¯­éŸ³è¯†åˆ«å™¨ä»£ç†
    self.speechRecognizer.delegate = self;
    
    // åˆå§‹åŒ–éŸ³é¢‘å¼•æ“
    self.audioEngine = [[AVAudioEngine alloc] init];
    
    // åˆå§‹åŒ–æ–‡æœ¬æ•°ç»„
    self.audioToTexts = [NSMutableArray array];
    
    // è¦æ±‚ç”¨æˆ·æˆäºˆæ‚¨çš„åº”ç”¨è®¸å¯æ¥æ‰§è¡Œè¯­éŸ³è¯†åˆ«ã€‚
    [WYSpeechRecognitionAuthorization authorizeSpeechRecognitionWithShowAlert:YES completionHandler:^(BOOL authorized) {
        [[NSOperationQueue mainQueue] addOperationWithBlock:^{
            self.textView.userInteractionEnabled = NO;
            self.voiceView.enabled = authorized;
        }];
    }];
}

- (void)startRecordingPersonSpeech {
    // æ£€æŸ¥ recognitionTask ä»»åŠ¡æ˜¯å¦å¤„äºè¿è¡ŒçŠ¶æ€ã€‚å¦‚æœæ˜¯ï¼Œå–æ¶ˆä»»åŠ¡å¼€å§‹æ–°çš„ä»»åŠ¡
    if (self.recognitionTask != nil) {
        // å–æ¶ˆå½“å‰è¯­éŸ³è¯†åˆ«ä»»åŠ¡ã€‚
        [self.recognitionTask cancel];
        self.recognitionTask = nil;
    }
    
    // å»ºç«‹ä¸€ä¸ªAVAudioSession ç”¨äºå½•éŸ³
    AVAudioSession *audioSession = [AVAudioSession sharedInstance];
    NSError *error = nil;
    @try {
        // category è®¾ç½®ä¸º record,å½•éŸ³
        [audioSession setCategory:AVAudioSessionCategoryRecord error:&error];
        if (error) {
            WYLog(@"audioSession setCategory error: %@", error);
            error = nil;
        }
        
        // mode è®¾ç½®ä¸º measurement
        [audioSession setMode:AVAudioSessionModeMeasurement error:&error];
        if (error) {
            WYLog(@"audioSession setMode error: %@", error);
            error = nil;
        }
        
        // å¼€å¯ audioSession
        [audioSession setActive:YES withOptions:AVAudioSessionSetActiveOptionNotifyOthersOnDeactivation error:&error];
        if (error) {
            WYLog(@"audioSession setActive error: %@", error);
            error = nil;
        }
    } @catch (NSException *exception) {
        WYLog(@"audioSession properties weren't set because of an error.");
    }
    
    // åˆå§‹åŒ–RecognitionRequestï¼Œåœ¨åè¾¹æˆ‘ä»¬ä¼šç”¨å®ƒå°†å½•éŸ³æ•°æ®è½¬å‘ç»™è‹¹æœæœåŠ¡å™¨
    self.recognitionRequest = [[SFSpeechAudioBufferRecognitionRequest alloc] init];
    
    // æ£€æŸ¥ iPhone æ˜¯å¦æœ‰æœ‰æ•ˆçš„å½•éŸ³è®¾å¤‡
    AVAudioInputNode *inputNode = self.audioEngine.inputNode;
    if (!inputNode) {
        WYLog(@"Audio engine has no input node");
        return;
    }
    
    if (!self.recognitionRequest) {
        WYLog(@"Unable to create an SFSpeechAudioBufferRecognitionRequest object");
        return;
    }
    
    // åœ¨ç”¨æˆ·è¯´è¯çš„åŒæ—¶ï¼Œå°†è¯†åˆ«ç»“æœåˆ†æ‰¹æ¬¡è¿”å›
    self.recognitionRequest.shouldReportPartialResults = YES;
    
    // æ·»åŠ æ ‡ç‚¹ç¬¦å·
    if (@available(iOS 16, *)) {
        self.recognitionRequest.addsPunctuation = YES;
    } else {
        // iOS16ä»¥ä¸‹éœ€è¦è‡ªå·±å¤„ç†æ ‡ç‚¹ç¬¦å·ï¼Œå»ºè®®å¯ä»¥å‚è€ƒï¼šPaddleSpeech
    }
    
    // é˜²æ­¢é€šè¿‡ç½‘ç»œå‘é€éŸ³é¢‘ï¼Œè¯†åˆ«å°†ä¸å†é‚£ä¹ˆå‡†ç¡®
    if ([self.speechRecognizer supportsOnDeviceRecognition]) {
        self.recognitionRequest.requiresOnDeviceRecognition = YES;
    }
    
    // ä½¿ç”¨recognitionTaskæ–¹æ³•å¼€å§‹è¯†åˆ«ï¼Œè¿™é‡Œæ¨èä»£ç†å®ç°æ–¹å¼ï¼Œé—­åŒ…æ–¹å¼æ— æ³•å°†å·²ç»è¯†åˆ«åˆ°çš„æ–‡æœ¬å’Œæ–°è¯†åˆ«åˆ°çš„æ–‡æœ¬è¿æ¥èµ·æ¥
    self.recognitionTask = [self.speechRecognizer recognitionTaskWithRequest:self.recognitionRequest delegate:self];
    
    /*
     self.recognitionTask = [self.speechRecognizer recognitionTaskWithRequest:self.recognitionRequest resultHandler:^(SFSpeechRecognitionResult * _Nullable result, NSError * _Nullable error) {
     __weak typeof(self) weakSelf = self;
     // ç”¨äºæ£€æŸ¥è¯†åˆ«æ˜¯å¦ç»“æŸ
     BOOL isFinal = NO;
     // å¦‚æœ result ä¸æ˜¯ nil,
     if (result != nil) {
     // å°† textView.text è®¾ç½®ä¸º result çš„æœ€ä½³éŸ³è¯‘
     weakSelf.textView.text = result.bestTranscription.formattedString ?: @"";
     
     // å¦‚æœ result æ˜¯æœ€ç»ˆï¼Œå°† isFinal è®¾ç½®ä¸º true
     isFinal = result.isFinal;
     }
     
     // å¦‚æœæ²¡æœ‰é”™è¯¯å‘ç”Ÿï¼Œæˆ–è€… result å·²ç»ç»“æŸï¼Œåœæ­¢audioEngine å½•éŸ³ï¼Œç»ˆæ­¢ recognitionRequest å’Œ recognitionTask
     if (error != nil || isFinal) {
     [weakSelf.audioEngine stop];
     [inputNode removeTapOnBus:0];
     
     weakSelf.recognitionRequest = nil;
     weakSelf.recognitionTask = nil;
     // å¼€å§‹å½•éŸ³æŒ‰é’®å¯ç”¨
     weakSelf.voiceView.enabled = YES;
     }
     }];
     */
    
    // å‘recognitionRequeståŠ å…¥ä¸€ä¸ªéŸ³é¢‘è¾“å…¥
    AVAudioFormat *recordingFormat = [inputNode outputFormatForBus:0];
    [inputNode installTapOnBus:0 bufferSize:1024 format:recordingFormat block:^(AVAudioPCMBuffer * _Nonnull buffer, AVAudioTime * _Nonnull when) {
        [self.recognitionRequest appendAudioPCMBuffer:buffer];
    }];
    
    [self.audioEngine prepare];
    
    @try {
        // å¼€å§‹å½•éŸ³
        [self.audioEngine startAndReturnError:&error];
        if (error) {
            WYLog(@"audioEngine couldn't start because of an error: %@", error);
        }
    } @catch (NSException *exception) {
        WYLog(@"audioEngine couldn't start because of an error.");
    }
    
    self.textView.text = @"è¯·è®²è¯...";
}

- (IBAction)startRecording:(UIButton *)sender {
    
    if (self.audioEngine.isRunning) {
        // åœæ­¢å½•éŸ³
        [self.audioEngine stop];
        // è¡¨ç¤ºéŸ³é¢‘æºå·²å®Œæˆï¼Œå¹¶ä¸”ä¸ä¼šå†å°†éŸ³é¢‘é™„åŠ åˆ°è¯†åˆ«è¯·æ±‚ã€‚
        [self.recognitionRequest endAudio];
        self.voiceView.enabled = NO;
        [self.voiceView setTitle:@"è¯­éŸ³è¯†åˆ«" forState:UIControlStateNormal];
    } else {
        [self startRecordingPersonSpeech];
        [self.voiceView setTitle:@"ç»“æŸ" forState:UIControlStateNormal];
    }
}

#pragma mark - SFSpeechRecognizerDelegate

- (void)speechRecognizer:(SFSpeechRecognizer *)speechRecognizer availabilityDidChange:(BOOL)available {
    // Handle availability changes
}

#pragma mark - SFSpeechRecognitionTaskDelegate

// Called when the task first detects speech in the source audio
- (void)speechRecognitionDidDetectSpeech:(SFSpeechRecognitionTask *)task {
    WYLog(@"Called when the task first detects speech in the source audio");
}

// Called for all recognitions, including non-final hypothesis
- (void)speechRecognitionTask:(SFSpeechRecognitionTask *)task didHypothesizeTranscription:(SFTranscription *)transcription {
    // åœ¨è¿™é‡Œå®ç°å³æ—¶è½¬è¯‘æ•ˆæœ
    NSString *currentText = [self.audioToTexts componentsJoinedByString:@""];
    self.textView.text = [currentText stringByAppendingString:transcription.formattedString];
}

// Called only for final recognitions of utterances. No more about the utterance will be reported
- (void)speechRecognitionTask:(SFSpeechRecognitionTask *)task didFinishRecognition:(SFSpeechRecognitionResult *)recognitionResult {
    // è¿™é‡Œæ˜¯è·å–æœ€ç»ˆçš„è¯†åˆ«ç»“æœï¼Œå¹¶ä¸”å°† textView.text è®¾ç½®ä¸º result çš„æœ€ä½³éŸ³è¯‘
    // æ·»åŠ æ ‡ç‚¹ç¬¦å·
    NSString *symbol = @"";
    if (@available(iOS 16, *)) {
    } else {
        symbol = @",";
    }
    NSString *finalText = [recognitionResult.bestTranscription.formattedString stringByAppendingString:symbol];
    [self.audioToTexts addObject:finalText];
    self.textView.text = [self.audioToTexts componentsJoinedByString:@""];
}

// Called when the task is no longer accepting new audio but may be finishing final processing
- (void)speechRecognitionTaskFinishedReadingAudio:(SFSpeechRecognitionTask *)task {
    WYLog(@"Called when the task is no longer accepting new audio but may be finishing final processing");
}

// Called when the task has been cancelled, either by client app, the user, or the system
- (void)speechRecognitionTaskWasCancelled:(SFSpeechRecognitionTask *)task {
    WYLog(@"Called when the task has been cancelled, either by client app, the user, or the system");
}

// Called when recognition of all requested utterances is finished.
// If successfully is false, the error property of the task will contain error information
- (void)speechRecognitionTask:(SFSpeechRecognitionTask *)task didFinishSuccessfully:(BOOL)successfully {
    
    [self.audioEngine stop];
    [self.audioEngine.inputNode removeTapOnBus:0];
    self.recognitionRequest = nil;
    self.recognitionTask = nil;
    [self.audioToTexts removeAllObjects];
    self.textView.text = @"è¯­éŸ³è¯†åˆ«æ­¥éª¤\n1ã€æŒ‰ä¸‹ è¯­éŸ³è¯†åˆ« æŒ‰é’®\n2ã€è¯­éŸ³è¯†åˆ«(è¯´å‡ºæƒ³è¦è¯†åˆ«çš„å†…å®¹)\n3ã€æŒ‰ä¸‹ ç»“æŸ æŒ‰é’®ç»“æŸè¯­éŸ³è¯†åˆ«";
    self.voiceView.enabled = YES;
}

- (void)dealloc {
    [WYNetworkStatus stopListening:@"SpeechRecognition"];
}


/*
 #pragma mark - Navigation
 
 // In a storyboard-based application, you will often want to do a little preparation before navigation
 - (void)prepareForSegue:(UIStoryboardSegue *)segue sender:(id)sender {
 // Get the new view controller using [segue destinationViewController].
 // Pass the selected object to the new view controller.
 }
 */

@end
